{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final feature selection\n",
    "\n",
    "After the best candidates were selected, we will now tune the model to get the best hyperparameters for the new set of features, using 'precision' scoring. We will use both Random Forest and XGBoost models. Then we will start optimizing the feature set directly according to the score formula (keeping high precision and low number of features). For this purpose, we have a custom scroring function form the `top20_scoring.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from top20_scoring import top_20_perc_scoring\n",
    "\n",
    "x_path = \"data/x_train.txt\"\n",
    "y_path = \"data/y_train.txt\"\n",
    "\n",
    "X = np.loadtxt(x_path)\n",
    "y = np.loadtxt(y_path)\n",
    "\n",
    "selected_features = [8, 100, 101, 102, 103, 104, 105, 285, 328, 351, 403]\n",
    "\n",
    "X = X[:, selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparameter tuning\n",
    "\n",
    "For this model we tune the following hyperparameters:\n",
    "- `eta` - learning rate\n",
    "- `max_depth` - maximum depth of the tree\n",
    "- `subsample` - subsample ratio of the training instances\n",
    "- `colsample_bytree` - subsample ratio of columns when constructing each tree\n",
    "- `gamma` - minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "- `lambda` - L2 regularization term on weights\n",
    "- `alpha` - L1 regularization term on weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20736 candidates, totalling 103680 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'eta': 0.1, 'gamma': 0, 'max_depth': 3, 'min_child_weight': 5, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "param_grid = {\n",
    "    \"eta\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7, 9],\n",
    "    \"subsample\": [0.7, 0.9, 1],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1],\n",
    "    \"gamma\": [0, 0.1, 1, 10],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 1],\n",
    "    \"reg_lambda\": [1, 0.1, 0.01],\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    model, param_grid, cv=5, scoring=\"precision\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline precision: 0.6555 (+/- 0.0179)\n",
      "Tuned precision: 0.7126 (+/- 0.0104)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "baseline_model = xgb.XGBClassifier()\n",
    "tuned_model = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "\n",
    "scores = cross_val_score(baseline_model, X, y, cv=5, scoring=\"precision\", n_jobs=-1)\n",
    "print(f\"Baseline precision: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "scores = cross_val_score(tuned_model, X, y, cv=5, scoring=\"precision\", n_jobs=-1)\n",
    "print(f\"Tuned precision: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.7270\n",
      "Tuned score: 0.7650\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "def score_model(model, X, y, skf):\n",
    "    sum = 0\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        score = top_20_perc_scoring(y_test, y_pred_proba)\n",
    "        sum += score\n",
    "\n",
    "    return sum / skf.n_splits\n",
    "\n",
    "\n",
    "baseline_score = score_model(baseline_model, X, y, skf)\n",
    "tuned_score = score_model(tuned_model, X, y, skf)\n",
    "\n",
    "print(f\"Baseline score: {baseline_score:.4f}\")\n",
    "print(f\"Tuned score: {tuned_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameter tuning\n",
    "\n",
    "For this model we tune the same parameters we tuned in the `rf_tuning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [50, 100, 250, 500],\n",
    "    \"max_depth\": [None, 5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10, 25],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 8, 16],\n",
    "    \"max_features\": [\"log2\", \"sqrt\", None],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"n_jobs\": [-1],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "search = GridSearchCV(\n",
    "    rf,\n",
    "    param_dist,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"precision\",\n",
    "    verbose=1,\n",
    ")\n",
    "search.fit(X, y)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline precision: 0.6759 (+/- 0.0157)\n",
      "Tuned precision: 0.6932 (+/- 0.0182)\n"
     ]
    }
   ],
   "source": [
    "baseline_model = RandomForestClassifier()\n",
    "tuned_model = RandomForestClassifier(**search.best_params_)\n",
    "\n",
    "scores = cross_val_score(baseline_model, X, y, cv=5, scoring=\"precision\", n_jobs=-1)\n",
    "print(f\"Baseline precision: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "scores = cross_val_score(tuned_model, X, y, cv=5, scoring=\"precision\", n_jobs=-1)\n",
    "print(f\"Tuned precision: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.7360\n",
      "Tuned score: 0.7480\n"
     ]
    }
   ],
   "source": [
    "baseline_model = RandomForestClassifier()\n",
    "tuned_model = RandomForestClassifier(**search.best_params_)\n",
    "\n",
    "baseline_score = score_model(baseline_model, X, y, skf)\n",
    "tuned_score = score_model(tuned_model, X, y, skf)\n",
    "\n",
    "print(f\"Baseline score: {baseline_score:.4f}\")\n",
    "print(f\"Tuned score: {tuned_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that both in our custom scoring and for the precision XGBoost resulted with the best score. Therefore, in the following part we will only focus on the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection methods revisited\n",
    "\n",
    "Now we will run selected methods once again, but this time only on the top features. We do that to check if preselection of features affects the rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 105: 0.1304\n",
      "Feature 100: 0.1271\n",
      "Feature 102: 0.1225\n",
      "Feature 8: 0.1105\n",
      "Feature 103: 0.1097\n",
      "Feature 101: 0.1005\n",
      "Feature 104: 0.0982\n",
      "Feature 403: 0.0561\n",
      "Feature 328: 0.0498\n",
      "Feature 285: 0.0479\n",
      "Feature 351: 0.0474\n"
     ]
    }
   ],
   "source": [
    "ranking = {}\n",
    "\n",
    "ranking[\"xgboost\"] = np.zeros(X.shape[1])\n",
    "\n",
    "xgboost = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "xgboost.fit(X, y)\n",
    "\n",
    "for i, idx in enumerate(np.argsort(xgboost.feature_importances_)[::-1]):\n",
    "    print(f\"Feature {selected_features[idx]}: {xgboost.feature_importances_[idx]:.4f}\")\n",
    "    ranking[\"xgboost\"][idx] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReliefF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 105: 0.0016\n",
      "Feature 102: 0.0015\n",
      "Feature 100: 0.0013\n",
      "Feature 8: 0.0013\n",
      "Feature 101: 0.0013\n",
      "Feature 103: 0.0012\n",
      "Feature 104: 0.0009\n",
      "Feature 285: 0.0006\n",
      "Feature 351: 0.0005\n",
      "Feature 328: 0.0004\n",
      "Feature 403: 0.0002\n"
     ]
    }
   ],
   "source": [
    "from skrebate import ReliefF\n",
    "\n",
    "ranking[\"relief\"] = np.zeros(X.shape[1])\n",
    "\n",
    "relief = ReliefF(n_neighbors=1.0, n_jobs=-1)\n",
    "relief.fit(X, y)\n",
    "\n",
    "for i, idx in enumerate(np.argsort(relief.feature_importances_)[::-1]):\n",
    "    print(f\"Feature {selected_features[idx]}: {relief.feature_importances_[idx]:.4f}\")\n",
    "    ranking[\"relief\"][idx] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 102: 1\n",
      "Feature 100: 2\n",
      "Feature 105: 3\n",
      "Feature 103: 4\n",
      "Feature 101: 5\n",
      "Feature 104: 6\n",
      "Feature 8: 7\n",
      "Feature 285: 8\n",
      "Feature 403: 9\n",
      "Feature 328: 10\n",
      "Feature 351: 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "ranking[\"rfe\"] = np.zeros(X.shape[1])\n",
    "\n",
    "rfe = RFE(\n",
    "    estimator=xgb.XGBClassifier(**grid_search.best_params_),\n",
    "    step=1,\n",
    "    verbose=0,\n",
    "    n_features_to_select=1,\n",
    ")\n",
    "rfe.fit(X, y)\n",
    "\n",
    "for i, idx in enumerate(np.argsort(rfe.ranking_)):\n",
    "    print(f\"Feature {selected_features[idx]}: {rfe.ranking_[idx]}\")\n",
    "    ranking[\"rfe\"][idx] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 102: 0.2726\n",
      "Feature 105: 0.2701\n",
      "Feature 100: 0.2676\n",
      "Feature 103: 0.2400\n",
      "Feature 101: 0.2306\n",
      "Feature 104: 0.1798\n",
      "Feature 8: 0.1173\n",
      "Feature 328: 0.0683\n",
      "Feature 403: 0.0572\n",
      "Feature 285: 0.0558\n",
      "Feature 351: 0.0383\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "ranking[\"shap\"] = np.zeros(X.shape[1])\n",
    "\n",
    "xgboost = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "xgboost.fit(X, y)\n",
    "\n",
    "explainer = shap.TreeExplainer(xgboost)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap_results = {}\n",
    "for i in range(X.shape[1]):\n",
    "    shap_results[i] = np.abs(shap_values[:, i]).mean(0)\n",
    "\n",
    "sorted_results = sorted(shap_results.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(X.shape[1]):\n",
    "    print(\n",
    "        f\"Feature {selected_features[sorted_results[i][0]]}: {sorted_results[i][1]:.4f}\"\n",
    "    )\n",
    "    ranking[\"shap\"][sorted_results[i][0]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal Information Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 8: 1.0000\n",
      "Feature 100: 1.0000\n",
      "Feature 101: 1.0000\n",
      "Feature 102: 1.0000\n",
      "Feature 103: 1.0000\n",
      "Feature 104: 1.0000\n",
      "Feature 105: 1.0000\n",
      "Feature 328: 1.0000\n",
      "Feature 403: 1.0000\n",
      "Feature 351: 1.0000\n",
      "Feature 285: 0.9951\n"
     ]
    }
   ],
   "source": [
    "from minepy import MINE\n",
    "\n",
    "ranking[\"mic\"] = np.zeros(X.shape[1])\n",
    "\n",
    "mine = MINE(alpha=1.0, c=15)\n",
    "\n",
    "mic_results = {}\n",
    "for i in range(X.shape[1]):\n",
    "    mine.compute_score(X[:, i], y)\n",
    "    mic_results[i] = mine.mic()\n",
    "\n",
    "sorted_mic_results = sorted(mic_results.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(X.shape[1]):\n",
    "    print(\n",
    "        f\"Feature {selected_features[sorted_mic_results[i][0]]}: {sorted_mic_results[i][1]:.4f}\"\n",
    "    )\n",
    "    ranking[\"mic\"][sorted_mic_results[i][0]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: ['xgboost', 'relief', 'rfe', 'shap', 'mic']\n",
      "Feature 102: 6.0\n",
      "Feature 100: 7.0\n",
      "Feature 105: 9.0\n",
      "Feature 8: 18.0\n",
      "Feature 101: 19.0\n",
      "Feature 103: 19.0\n",
      "Feature 104: 27.0\n",
      "Feature 328: 40.0\n",
      "Feature 403: 41.0\n",
      "Feature 285: 42.0\n",
      "Feature 351: 47.0\n"
     ]
    }
   ],
   "source": [
    "final_ranking = np.sum(list(ranking.values()), axis=0)\n",
    "print(f\"Methods: {list(ranking.keys())}\")\n",
    "\n",
    "for i in np.argsort(final_ranking):\n",
    "    print(f\"Feature {selected_features[i]}: {final_ranking[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now ranking differ a bit. Features 102, 100, and 105 scored very good, features 8, 101, 103 and 104 were a bit worse, and all the rest were mostly on the bottom of the ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net income based selection\n",
    "\n",
    "Even though we get some new feature importance ranking, we won't reduce it further on this basis, and instead just select the most optimal combination, based on the score calculated according to the task's formula. To make results reliable we'll use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "def score_subset(X, skf, best_params):\n",
    "    n_features = X.shape[1]\n",
    "    n_splits = skf.n_splits\n",
    "\n",
    "    net_income = -200 * n_features * n_splits\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = xgb.XGBClassifier(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        TP = top_20_perc_scoring(y_test, y_pred_proba) * X.shape[0] * 0.2\n",
    "        net_income += TP * 10\n",
    "\n",
    "    return net_income / n_splits\n",
    "\n",
    "subsets = []\n",
    "\n",
    "for num_features in range(1, X.shape[1] + 1):\n",
    "    for subset in combinations(range(X.shape[1]), num_features):\n",
    "        subset = list(subset)\n",
    "        X_subset = X[:, subset]\n",
    "        score = score_subset(X_subset, skf, grid_search.best_params_)\n",
    "        subsets.append({\n",
    "            \"subset\": [selected_features[i] for i in subset],\n",
    "            \"score\": score\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 102, 103, 105]</td>\n",
       "      <td>6930.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[8, 100, 102, 103]</td>\n",
       "      <td>6840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[102, 103, 105]</td>\n",
       "      <td>6820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 102, 103]</td>\n",
       "      <td>6820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 102, 103]</td>\n",
       "      <td>6810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>[285, 351]</td>\n",
       "      <td>4700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>[285, 351, 403]</td>\n",
       "      <td>4690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>[285, 328, 351]</td>\n",
       "      <td>4620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>[285, 328, 351, 403]</td>\n",
       "      <td>4610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>[285]</td>\n",
       "      <td>4600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2047 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subset   score\n",
       "0     [101, 102, 103, 105]  6930.0\n",
       "1       [8, 100, 102, 103]  6840.0\n",
       "2          [102, 103, 105]  6820.0\n",
       "3          [100, 102, 103]  6820.0\n",
       "4          [101, 102, 103]  6810.0\n",
       "...                    ...     ...\n",
       "2042            [285, 351]  4700.0\n",
       "2043       [285, 351, 403]  4690.0\n",
       "2044       [285, 328, 351]  4620.0\n",
       "2045  [285, 328, 351, 403]  4610.0\n",
       "2046                 [285]  4600.0\n",
       "\n",
       "[2047 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "subsets_df = pd.DataFrame(subsets)\n",
    "subsets_df = subsets_df.sort_values(\"score\", ascending=False)\n",
    "subsets_df = subsets_df.reset_index(drop=True)\n",
    "\n",
    "subsets_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the subset providing the best compromise between precision and number of features is [101, 102, 103, 105]. We'll use that set for the final model. Interesting fact is that, it seems that those features outside of the group 0-9 and 100-105 are not very important for the model. This is an important insight - the feature importance methods are not ideal and sometimes they can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 102, 103, 105]\n"
     ]
    }
   ],
   "source": [
    "best_subset = subsets_df.iloc[0].subset\n",
    "print(best_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20736 candidates, totalling 103680 fits\n",
      "{'colsample_bytree': 0.7, 'eta': 0.01, 'gamma': 0.1, 'max_depth': 5, 'min_child_weight': 5, 'reg_alpha': 1, 'reg_lambda': 0.1, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "X = np.loadtxt(\"data/x_train.txt\")\n",
    "X = X[:, best_subset]\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"eta\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7, 9],\n",
    "    \"subsample\": [0.7, 0.9, 1],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1],\n",
    "    \"gamma\": [0, 0.1, 1, 10],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 1],\n",
    "    \"reg_lambda\": [1, 0.1, 0.01],\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    model, param_grid, cv=5, scoring=\"precision\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, after even further feature selection, and change to our custom scoring function (which still heavily relies on precision), the most optimal hyperparameters changed. Let's check the improvement in scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final precision: 0.6932 (+/- 0.0182)\n",
      "Final score: 0.7460\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "final_model = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "\n",
    "score = cross_val_score(final_model, X, y, cv=5, scoring=\"precision\", n_jobs=-1)\n",
    "print(f\"Final precision: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "final_model = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "\n",
    "score = score_model(final_model, X, y, skf)\n",
    "print(f\"Final score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 4 features we got a score and precision close to the top 12 features subset model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge submission\n",
    "\n",
    "With the final model and optimal feature selection, now we generate the predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.loadtxt(\"data/x_test.txt\")\n",
    "X_test = X_test[:, best_subset]\n",
    "\n",
    "grid_search.best_params_ = {'colsample_bytree': 0.7, 'eta': 0.01, 'gamma': 0.1, 'max_depth': 5, 'min_child_weight': 5, 'reg_alpha': 1, 'reg_lambda': 0.1, 'subsample': 0.9}\n",
    "final_model = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "top_1000_idx = np.argsort(y_pred_proba)[::-1][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_path = \"data/vars.txt\"\n",
    "obs_path = \"data/obs.txt\"\n",
    "\n",
    "np.savetxt(vars_path, [i + 1 for i in best_subset], fmt=\"%d\")\n",
    "np.savetxt(obs_path, top_1000_idx + 1, fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
