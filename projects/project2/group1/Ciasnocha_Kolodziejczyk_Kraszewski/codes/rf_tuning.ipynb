{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model: Random Forest\n",
    "\n",
    "Since some feature selection methods require a model to be trained, we will use Random Forest for this task. To make the model more robust, we will use cross-validation to tune it on all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_path = \"data/x_train.txt\"\n",
    "y_path = \"data/y_train.txt\"\n",
    "\n",
    "X = np.loadtxt(x_path)\n",
    "y = np.loadtxt(y_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning\n",
    "\n",
    "There are several features worth tuning in Random Forest. We will use cross-validation to tune the following hyperparameters:\n",
    "- `n_estimators`: the number of trees in the forest\n",
    "- `max_depth`: the maximum depth of the tree\n",
    "- `min_samples_split`: the minimum number of samples required to split an internal node\n",
    "- `min_samples_leaf`: the minimum number of samples required to be at a leaf node\n",
    "- `max_features`: the number of features to consider when looking for the best split\n",
    "- `bootstrap`: whether bootstrap samples are used when building trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "648 fits failed out of a total of 1944.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "188 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "460 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1052: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.59654356 0.62281941 0.62785384\n",
      " 0.60105977 0.62581094 0.63804959 0.60569701 0.6247765  0.6434179\n",
      " 0.59401521 0.62026633 0.63797848 0.59289894 0.62642421 0.64076028\n",
      " 0.60995957 0.6379518  0.63685058 0.60747688 0.62322931 0.64022192\n",
      " 0.60747688 0.62322931 0.64022192 0.60171046 0.62645913 0.63704497\n",
      " 0.64597066 0.64582661 0.64752662 0.64955175 0.64783446 0.64859402\n",
      " 0.64282441 0.64882461 0.64903096 0.64987318 0.64905333 0.64710935\n",
      " 0.64831504 0.64543426 0.64931962 0.64997513 0.64684744 0.64930384\n",
      " 0.65054199 0.64614173 0.64939329 0.65054199 0.64614173 0.64939329\n",
      " 0.64922685 0.6449117  0.6496231         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63366001 0.64360876 0.65016901 0.63393846 0.64049159 0.64635908\n",
      " 0.64746561 0.64144563 0.6435927  0.63506671 0.64206872 0.6430284\n",
      " 0.63697408 0.64375025 0.64843568 0.6458472  0.6402522  0.64846029\n",
      " 0.63970389 0.6387845  0.65145021 0.63970389 0.6387845  0.65145021\n",
      " 0.64423647 0.64525298 0.65096072 0.64938903 0.65174326 0.65404473\n",
      " 0.64904211 0.6521877  0.65375833 0.64936809 0.65101088 0.65292684\n",
      " 0.64911678 0.6526845  0.65299069 0.64893512 0.65223716 0.6528465\n",
      " 0.64875485 0.65114635 0.65307202 0.65016779 0.65142119 0.65321203\n",
      " 0.65016779 0.65142119 0.65321203 0.64985677 0.65065594 0.65233256\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.61029063 0.63403719 0.64516947\n",
      " 0.62875609 0.64612494 0.65076065 0.61841072 0.63881279 0.6455965\n",
      " 0.61781863 0.6381205  0.65294986 0.62308388 0.63904662 0.65095033\n",
      " 0.62425677 0.64227149 0.64798347 0.62442302 0.64408596 0.64718016\n",
      " 0.62442302 0.64408596 0.64718016 0.62398374 0.64613569 0.6482711\n",
      " 0.65016281 0.64901591 0.65087624 0.65274194 0.64705561 0.64910763\n",
      " 0.64762263 0.64859345 0.65028228 0.65264175 0.64732678 0.64911343\n",
      " 0.6521067  0.64675729 0.64868511 0.64753166 0.6466621  0.64962141\n",
      " 0.65147051 0.64849705 0.65047326 0.65147051 0.64849705 0.65047326\n",
      " 0.65110073 0.64763452 0.65035339        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62176643 0.64130224 0.64729409 0.60415697 0.63493547 0.65861104\n",
      " 0.61530429 0.63562916 0.64710526 0.61227569 0.63669321 0.64633799\n",
      " 0.61310121 0.63801329 0.64899948 0.61272003 0.63811149 0.64955424\n",
      " 0.62094357 0.63734354 0.63967621 0.62094357 0.63734354 0.63967621\n",
      " 0.62150223 0.63001299 0.6462886  0.65582453 0.65288827 0.6562196\n",
      " 0.65523439 0.65314703 0.6556793  0.6511161  0.65156479 0.65467886\n",
      " 0.65316147 0.65705078 0.65467353 0.65323705 0.65096893 0.65470209\n",
      " 0.65401087 0.6518117  0.65646192 0.65398549 0.65197565 0.65371639\n",
      " 0.65398549 0.65197565 0.65371639 0.65084168 0.65238159 0.65495175\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.63261259 0.63355609 0.63934193\n",
      " 0.61589645 0.62679226 0.63992407 0.61237611 0.63013518 0.64222427\n",
      " 0.61138555 0.63236999 0.63521633 0.61510191 0.62764158 0.62823668\n",
      " 0.61571463 0.62637752 0.63309803 0.62553635 0.63071108 0.63489268\n",
      " 0.62553635 0.63071108 0.63489268 0.62717605 0.63677808 0.64357307\n",
      " 0.56423961 0.56640206 0.56605122 0.56436564 0.56183764 0.56463919\n",
      " 0.56384423 0.56243006 0.56090261 0.56509426 0.5659292  0.56512978\n",
      " 0.56264163 0.56358213 0.56352632 0.56203282 0.56444517 0.56308412\n",
      " 0.57538292 0.57557603 0.57453512 0.57538292 0.57557603 0.57453512\n",
      " 0.57448343 0.57254801 0.57414954        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62672757 0.63663636 0.6384389  0.62800391 0.63630151 0.64230433\n",
      " 0.63268254 0.64071072 0.6423147  0.62720735 0.63733394 0.6410192\n",
      " 0.63021122 0.63724537 0.64067414 0.63502926 0.64071563 0.64127478\n",
      " 0.6366283  0.63571246 0.63642267 0.6366283  0.63571246 0.63642267\n",
      " 0.63493706 0.63484622 0.63777633 0.61844464 0.61844464 0.61866175\n",
      " 0.61844464 0.61844464 0.61866175 0.61844464 0.61844464 0.61866175\n",
      " 0.61844464 0.61844464 0.61866175 0.61844464 0.61844464 0.61866175\n",
      " 0.61844464 0.61844464 0.61866175 0.61842376 0.61842376 0.61864087\n",
      " 0.61842376 0.61842376 0.61864087 0.61842376 0.61842376 0.61864087\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.62190888 0.63504995 0.64466401\n",
      " 0.63552699 0.64227799 0.64671859 0.6310103  0.64067085 0.64626954\n",
      " 0.62114375 0.63715745 0.64550673 0.62557169 0.64910126 0.64129393\n",
      " 0.63063067 0.63742174 0.64767762 0.6316589  0.63413453 0.64235118\n",
      " 0.6316589  0.63413453 0.64235118 0.63027594 0.63540926 0.64501939\n",
      " 0.62834018 0.62883927 0.62823082 0.62986757 0.62811927 0.62839627\n",
      " 0.62799338 0.62783731 0.62695609 0.62800628 0.62786557 0.62855097\n",
      " 0.62934291 0.62968931 0.62863862 0.62829919 0.62805343 0.62754505\n",
      " 0.6315466  0.63006701 0.63004655 0.6315466  0.63006701 0.63004655\n",
      " 0.6302232  0.63071813 0.63040855        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62234327 0.63505469 0.64435018 0.61572448 0.63605463 0.63899646\n",
      " 0.62744108 0.63462544 0.64230798 0.62208901 0.63785922 0.63904573\n",
      " 0.61832534 0.63436404 0.64172466 0.62263167 0.62998332 0.63838876\n",
      " 0.61942573 0.63379253 0.6422037  0.61942573 0.63379253 0.6422037\n",
      " 0.63640274 0.64136916 0.63915343 0.617042   0.61719033 0.61624978\n",
      " 0.61654402 0.61577946 0.61647705 0.61569265 0.61518799 0.61430155\n",
      " 0.61672836 0.61819259 0.61631012 0.61538615 0.61601663 0.61571487\n",
      " 0.61547242 0.61753671 0.6171474  0.62084184 0.62046686 0.61955834\n",
      " 0.62084184 0.62046686 0.61955834 0.62040651 0.620291   0.62030306]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 250, 500],\n",
    "    \"max_depth\": [None, 5, 10, 15],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"log2\", \"sqrt\", None],\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "search = GridSearchCV(\n",
    "    rf,\n",
    "    param_dist,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"precision\",\n",
    "    verbose=1,\n",
    ")\n",
    "search.fit(X, y)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there was a small error in the `max_features` parameter (\"auto\" instead of \"log2\"), we will do an extra tuning for this parameter instead of repeating the whole process (very time-consuming). We also reduced some of the ranges, setting the best value from the previous stage (note it has some risk of missing the global optimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "{'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    \"n_estimators\": [250, 500],\n",
    "    \"max_depth\": [None, 10, 15],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"log2\", \"sqrt\"],\n",
    "    \"bootstrap\": [True],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "search = GridSearchCV(\n",
    "    rf,\n",
    "    param_dist,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"precision\",\n",
    "    verbose=1,\n",
    ")\n",
    "search.fit(X, y)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model vs tuned model\n",
    "\n",
    "After obtaining the best hyperparameters, we will ensure that the model is indeed better, comparing it with the model with default hyperparameters using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline precision: 0.6093 (+/- 0.0278)\n",
      "Tuned precision: 0.6504 (+/- 0.0204)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "baseline_model = RandomForestClassifier(random_state=42)\n",
    "scores = cross_val_score(baseline_model, X, y, cv=5, scoring=\"precision\", n_jobs=-1)\n",
    "print(f\"Baseline precision: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "tuned_model = RandomForestClassifier(**search.best_params_)\n",
    "scores = cross_val_score(tuned_model, X, y, cv=5, scoring=\"precision\", n_jobs=-1)\n",
    "print(f\"Tuned precision: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model indeed achieved better results, so we will use it for feature selection. Since the main goal of the task is to have the best precision in top 20% of predictions, we will use custom scoring function for one more comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/advanced-ml/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.6710\n",
      "Tuned score: 0.6880\n"
     ]
    }
   ],
   "source": [
    "from top20_scoring import top_20_perc_scoring\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "def score_model(model, X, y, skf):\n",
    "    sum = 0\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        score = top_20_perc_scoring(y_test, y_pred_proba)\n",
    "        sum += score\n",
    "\n",
    "    return sum / skf.n_splits\n",
    "\n",
    "baseline_score = score_model(baseline_model, X, y, skf)\n",
    "tuned_score = score_model(tuned_model, X, y, skf)\n",
    "\n",
    "print(f\"Baseline score: {baseline_score:.4f}\")\n",
    "print(f\"Tuned score: {tuned_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here once again the model with tuned hyperparameters outperformed the baseline model, but the difference is not that significant. It could be expected since we only select the most probable part of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
